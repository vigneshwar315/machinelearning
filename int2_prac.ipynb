{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5mLosr9F+73pv76I1AAGF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vigneshwar315/machinelearning/blob/main/int2_prac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRggJwGKXuBZ",
        "outputId": "6da038db-dfd0-47ff-bfae-fcbfd920775c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Decision Tree Evaluation ===\n",
            "Accuracy : 0.8167\n",
            "Precision: 0.7607\n",
            "Recall   : 0.8857\n",
            "F1-score : 0.8185\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.76      0.81       160\n",
            "           1       0.76      0.89      0.82       140\n",
            "\n",
            "    accuracy                           0.82       300\n",
            "   macro avg       0.82      0.82      0.82       300\n",
            "weighted avg       0.83      0.82      0.82       300\n",
            "\n",
            "\n",
            "=== Random Forest Evaluation ===\n",
            "Accuracy : 0.9167\n",
            "Precision: 0.8710\n",
            "Recall   : 0.9643\n",
            "F1-score : 0.9153\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.88      0.92       160\n",
            "           1       0.87      0.96      0.92       140\n",
            "\n",
            "    accuracy                           0.92       300\n",
            "   macro avg       0.92      0.92      0.92       300\n",
            "weighted avg       0.92      0.92      0.92       300\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#write a program that demonstrates the advantage of ensemble learning compared to a single classifier. apply a decision tree and a random forest classifier on a given dataset(make use of the synthetic data). compare their evaluations using evluation metrics such as accuracy precsion recall and f1 score.\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "X, y = make_classification(n_samples=1000,n_features=20,n_informative=10,n_redundant=5,n_classes=2,random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"\\n=== {model_name} Evaluation ===\")\n",
        "    print(f\"Accuracy : {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Recall   : {recall_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"F1-score : {f1_score(y_true, y_pred):.4f}\")\n",
        "    print(\"\\nDetailed Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "evaluate_model(y_test, y_pred_dt, \"Decision Tree\")\n",
        "evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#write a program that demonstrates the use of simple ensemble techniques :Max Voting, average Voting and weighted average voting (assign weghts based oneach model performance\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,n_redundant=5, n_classes=2, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model1 = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model2 = DecisionTreeClassifier(random_state=42)\n",
        "model3 = SVC(probability=True, random_state=42)\n",
        "\n",
        "for model in (model1, model2, model3):\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "pred1 = model1.predict(X_test)\n",
        "pred2 = model2.predict(X_test)\n",
        "pred3 = model3.predict(X_test)\n",
        "\n",
        "preds = np.array([pred1, pred2, pred3])\n",
        "hard_voting_pred = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=preds)\n",
        "\n",
        "\n",
        "prob1 = model1.predict_proba(X_test)\n",
        "prob2 = model2.predict_proba(X_test)\n",
        "prob3 = model3.predict_proba(X_test)\n",
        "avg_prob = (prob1 + prob2 + prob3) / 3\n",
        "soft_voting_pred = np.argmax(avg_prob, axis=1)\n",
        "\n",
        "acc1 = accuracy_score(y_test, pred1)\n",
        "acc2 = accuracy_score(y_test, pred2)\n",
        "acc3 = accuracy_score(y_test, pred3)\n",
        "\n",
        "weights = np.array([acc1, acc2, acc3])\n",
        "weights = weights / weights.sum()\n",
        "\n",
        "weighted_avg_prob = (prob1 * weights[0] + prob2 * weights[1] + prob3 * weights[2])\n",
        "weighted_voting_pred = np.argmax(weighted_avg_prob, axis=1)\n",
        "\n",
        "\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"\\n=== {model_name} ===\")\n",
        "    print(f\"Accuracy : {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(classification_report(y_true, y_pred, digits=3))\n",
        "\n",
        "evaluate_model(y_test, pred1, \"Logistic Regression\")\n",
        "evaluate_model(y_test, pred2, \"Decision Tree\")\n",
        "evaluate_model(y_test, pred3, \"SVM\")\n",
        "\n",
        "evaluate_model(y_test, hard_voting_pred, \"Hard Voting (Max Voting)\")\n",
        "evaluate_model(y_test, soft_voting_pred, \"Soft Voting (Average Voting)\")\n",
        "evaluate_model(y_test, weighted_voting_pred, \"Weighted Average Voting\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N02KUGthpux",
        "outputId": "19877a3a-8df8-4a16-b9e1-319c300bbe0f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression ===\n",
            "Accuracy : 0.8367\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.878     0.806     0.840       160\n",
            "           1      0.797     0.871     0.833       140\n",
            "\n",
            "    accuracy                          0.837       300\n",
            "   macro avg      0.837     0.839     0.837       300\n",
            "weighted avg      0.840     0.837     0.837       300\n",
            "\n",
            "\n",
            "=== Decision Tree ===\n",
            "Accuracy : 0.8167\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.883     0.756     0.815       160\n",
            "           1      0.761     0.886     0.818       140\n",
            "\n",
            "    accuracy                          0.817       300\n",
            "   macro avg      0.822     0.821     0.817       300\n",
            "weighted avg      0.826     0.817     0.817       300\n",
            "\n",
            "\n",
            "=== SVM ===\n",
            "Accuracy : 0.9500\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.980     0.925     0.952       160\n",
            "           1      0.919     0.979     0.948       140\n",
            "\n",
            "    accuracy                          0.950       300\n",
            "   macro avg      0.950     0.952     0.950       300\n",
            "weighted avg      0.952     0.950     0.950       300\n",
            "\n",
            "\n",
            "=== Hard Voting (Max Voting) ===\n",
            "Accuracy : 0.9233\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.960     0.894     0.926       160\n",
            "           1      0.887     0.957     0.921       140\n",
            "\n",
            "    accuracy                          0.923       300\n",
            "   macro avg      0.924     0.925     0.923       300\n",
            "weighted avg      0.926     0.923     0.923       300\n",
            "\n",
            "\n",
            "=== Soft Voting (Average Voting) ===\n",
            "Accuracy : 0.9167\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.947     0.894     0.920       160\n",
            "           1      0.886     0.943     0.913       140\n",
            "\n",
            "    accuracy                          0.917       300\n",
            "   macro avg      0.916     0.918     0.917       300\n",
            "weighted avg      0.919     0.917     0.917       300\n",
            "\n",
            "\n",
            "=== Weighted Average Voting ===\n",
            "Accuracy : 0.9233\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.960     0.894     0.926       160\n",
            "           1      0.887     0.957     0.921       140\n",
            "\n",
            "    accuracy                          0.923       300\n",
            "   macro avg      0.924     0.925     0.923       300\n",
            "weighted avg      0.926     0.923     0.923       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Write a program to show the difference between Hard Voting and Soft Voting classifiers in ensemble learning using multiple base learners (e.g., Decision Tree, Logistic Regression, and KNN).\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5,n_classes=2, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model1 = DecisionTreeClassifier(random_state=42)\n",
        "model2 = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model3 = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "hard_voting = VotingClassifier(estimators=[('dt', model1), ('lr', model2), ('knn', model3)],voting='hard')\n",
        "\n",
        "soft_voting = VotingClassifier(estimators=[('dt', model1), ('lr', model2), ('knn', model3)],voting='soft' )\n",
        "\n",
        "hard_voting.fit(X_train, y_train)\n",
        "soft_voting.fit(X_train, y_train)\n",
        "\n",
        "y_pred_hard = hard_voting.predict(X_test)\n",
        "y_pred_soft = soft_voting.predict(X_test)\n",
        "\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"\\n=== {model_name} ===\")\n",
        "    print(f\"Accuracy : {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(classification_report(y_true, y_pred, digits=3))\n",
        "\n",
        "for name, model in [('Decision Tree', model1), ('Logistic Regression', model2), ('KNN', model3)]:\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    evaluate_model(y_test, preds, name)\n",
        "\n",
        "evaluate_model(y_test, y_pred_hard, \"Hard Voting Classifier\")\n",
        "evaluate_model(y_test, y_pred_soft, \"Soft Voting Classifier\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxES8O5Rlsrq",
        "outputId": "7fe511e7-c1ce-4818-c6fb-c9a898886019"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Decision Tree ===\n",
            "Accuracy : 0.8167\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.883     0.756     0.815       160\n",
            "           1      0.761     0.886     0.818       140\n",
            "\n",
            "    accuracy                          0.817       300\n",
            "   macro avg      0.822     0.821     0.817       300\n",
            "weighted avg      0.826     0.817     0.817       300\n",
            "\n",
            "\n",
            "=== Logistic Regression ===\n",
            "Accuracy : 0.8367\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.878     0.806     0.840       160\n",
            "           1      0.797     0.871     0.833       140\n",
            "\n",
            "    accuracy                          0.837       300\n",
            "   macro avg      0.837     0.839     0.837       300\n",
            "weighted avg      0.840     0.837     0.837       300\n",
            "\n",
            "\n",
            "=== KNN ===\n",
            "Accuracy : 0.9167\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.919     0.925     0.922       160\n",
            "           1      0.914     0.907     0.910       140\n",
            "\n",
            "    accuracy                          0.917       300\n",
            "   macro avg      0.916     0.916     0.916       300\n",
            "weighted avg      0.917     0.917     0.917       300\n",
            "\n",
            "\n",
            "=== Hard Voting Classifier ===\n",
            "Accuracy : 0.9233\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.954     0.900     0.926       160\n",
            "           1      0.893     0.950     0.920       140\n",
            "\n",
            "    accuracy                          0.923       300\n",
            "   macro avg      0.923     0.925     0.923       300\n",
            "weighted avg      0.925     0.923     0.923       300\n",
            "\n",
            "\n",
            "=== Soft Voting Classifier ===\n",
            "Accuracy : 0.9067\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.940     0.881     0.910       160\n",
            "           1      0.873     0.936     0.903       140\n",
            "\n",
            "    accuracy                          0.907       300\n",
            "   macro avg      0.907     0.908     0.907       300\n",
            "weighted avg      0.909     0.907     0.907       300\n",
            "\n"
          ]
        }
      ]
    }
  ]
}